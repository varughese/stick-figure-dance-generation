{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, codecs\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We preprocess'd the file names so we have a index with a list of all frames per dance id\n",
    "FRAME_LIST_INDEX = './dance-frame-list.json'\n",
    "\n",
    "# np.random.seed(0)\n",
    "NUM_BODY_PARTS = 13\n",
    "TOTAL_FRAMES = 250\n",
    "\n",
    "# We have 250 frames. We are going to going to take the 17 body parts, \n",
    "# and turn it into 13 (remove eyes and ears). Then 13x2 (13 body parts, 2 vectors)\n",
    "def from_motion_to_numpy_vector(motion):\n",
    "    # For now, we only take the first person. Later we can maybe try to feed in all people, or do batches of two\n",
    "    motion_vector = np.zeros((250, NUM_BODY_PARTS, 2))\n",
    "    if len(motion) < 250:\n",
    "        print(\"We need 250 frames.\")\n",
    "    for i, frame in enumerate(motion):\n",
    "        if len(frame) > 0 and i < TOTAL_FRAMES:\n",
    "            current_frame_data = frame\n",
    "            # TODO extend this past just 1 person\n",
    "            person0 = current_frame_data[0][1:]\n",
    "            current_frame_vector = np.zeros((NUM_BODY_PARTS, 2))\n",
    "            current_body_part_idx = 0\n",
    "            for body_part_data in person0:\n",
    "                body_part = body_part_data[0]\n",
    "                if body_part not in ['left_eye', 'left_ear', 'right_eye', 'right_ear']:\n",
    "                    current_frame_vector[current_body_part_idx] = body_part_data[1]\n",
    "                    current_body_part_idx = current_body_part_idx + 1\n",
    "            motion_vector[i] = current_frame_vector\n",
    "    return motion_vector\n",
    "\n",
    "def from_numpy_vector_to_motion_coordinates(motion_vector):\n",
    "    # Reshape so each element in array is an a NUM_BODY_PARTS x 2 array that has coordinates\n",
    "    return motion_vector.reshape(TOTAL_FRAMES, NUM_BODY_PARTS, 2)\n",
    "\n",
    "class LetsDanceDataset(torch.utils.data.Dataset):\n",
    "    categories_hash = {'tango': 0, 'break': 1, 'swing': 2,'quickstep': 3,\n",
    "                  'foxtrot': 4,'pasodoble': 5,'tap': 6,'samba': 7,'flamenco': 8,\n",
    "                  'ballet': 9,'rumba': 10,'waltz': 11,'cha': 12,'latin': 13,\n",
    "                  'square': 14,'jive': 15}\n",
    "    \n",
    "    def __init__(self, root_dir, dances):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.data = np.zeros((len(dances), TOTAL_FRAMES, NUM_BODY_PARTS, 2))\n",
    "        self.metadata = dances\n",
    "        \n",
    "        dances = list(filter(lambda dance: dance[2] >= TOTAL_FRAMES, dances))\n",
    "        \n",
    "        for i, dance in enumerate(dances):\n",
    "            [category, dance_id, frames] = dance\n",
    "            current_frame_path = \"{}{}/{}.json\".format(root_dir, category, dance_id)\n",
    "            with open(current_frame_path) as f:\n",
    "                motion = json.load(f)\n",
    "            self.data[i] = from_motion_to_numpy_vector(motion)\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def getitem_metadata(self, index):\n",
    "        return self.metadata[index]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns (category, motion)\n",
    "        motion is in shape of `(NUM_FRAMES, 13, 2)`\n",
    "        '''\n",
    "        return torch.Tensor(self.data[index])\n",
    "\n",
    "    def get_num_body_parts(self):\n",
    "        return NUM_BODY_PARTS * 2 # x + y # FIX\n",
    "\n",
    "    def save_data(self, filename, np_array):\n",
    "        # TODO \n",
    "        file_path = 'data/' + filename + \".json\"\n",
    "        d = np_array.tolist()\n",
    "#         d = from_numpy_vector_to_motion_coordinates(np_array).tolist()\n",
    "        json.dump(d, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True)\n",
    "    \n",
    "\n",
    "\n",
    "# For this first test, we are just using Latin dances\n",
    "with open(FRAME_LIST_INDEX) as f:\n",
    "    frames_index = json.load(f)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(frames_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 13, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dances= frames_index[:10]\n",
    "# valid_dances = frames_index[1000:]\n",
    "train_dataset = LetsDanceDataset('../densepose/full/', train_dances)\n",
    "# valid_dataloader = LetsDanceDataset('../densepose/full/', latin_dances[40:])\n",
    "train_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.savve_data('test_test_test',train_dataset[3] )\n",
    "# train_dataset[3].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from c_rnn_gan import Generator, Discriminator\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "CKPT_DIR = 'models'\n",
    "\n",
    "G_FN = 'c_rnn_gan_g.pth'\n",
    "D_FN = 'c_rnn_gan_d.pth'\n",
    "\n",
    "G_LRN_RATE = 0.001\n",
    "D_LRN_RATE = 0.001\n",
    "MAX_GRAD_NORM = 5.0\n",
    "# following values are modified at runtime\n",
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "EPSILON = 1e-40 # value to use to approximate zero (to prevent undefined results)\n",
    "\n",
    "class GLoss(nn.Module):\n",
    "    ''' C-RNN-GAN generator loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(GLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits_gen):\n",
    "        logits_gen = torch.clamp(logits_gen, EPSILON, 1.0)\n",
    "        batch_loss = -torch.log(logits_gen)\n",
    "\n",
    "        return torch.mean(batch_loss)\n",
    "\n",
    "\n",
    "class DLoss(nn.Module):\n",
    "    ''' C-RNN-GAN discriminator loss\n",
    "    '''\n",
    "    def __init__(self, label_smoothing=False):\n",
    "        super(DLoss, self).__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, logits_real, logits_gen):\n",
    "        ''' Discriminator loss\n",
    "\n",
    "        logits_real: logits from D, when input is real\n",
    "        logits_gen: logits from D, when input is from Generator\n",
    "\n",
    "        loss = -(ylog(p) + (1-y)log(1-p))\n",
    "\n",
    "        '''\n",
    "        logits_real = torch.clamp(logits_real, EPSILON, 1.0)\n",
    "        d_loss_real = -torch.log(logits_real)\n",
    "\n",
    "        if self.label_smoothing:\n",
    "            p_fake = torch.clamp((1 - logits_real), EPSILON, 1.0)\n",
    "            d_loss_fake = -torch.log(p_fake)\n",
    "            d_loss_real = 0.9*d_loss_real + 0.1*d_loss_fake\n",
    "\n",
    "        logits_gen = torch.clamp((1 - logits_gen), EPSILON, 1.0)\n",
    "        d_loss_gen = -torch.log(logits_gen)\n",
    "\n",
    "        batch_loss = d_loss_real + d_loss_gen\n",
    "        return torch.mean(batch_loss)\n",
    "\n",
    "\n",
    "def run_training(model, optimizer, criterion, dataloader, freeze_g=False, freeze_d=False):\n",
    "    ''' Run single training epoch\n",
    "    '''\n",
    "    \n",
    "    num_feats = train_dataset.get_num_body_parts()\n",
    "    # dataloader.rewind(part='train')\n",
    "    # batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='train')\n",
    "\n",
    "    model['g'].train()\n",
    "    model['d'].train()\n",
    "\n",
    "    loss = {}\n",
    "    g_loss_total = 0.0\n",
    "    d_loss_total = 0.0\n",
    "    num_corrects = 0\n",
    "    num_sample = 0\n",
    "\n",
    "    for step, dance in enumerate(dataloader):\n",
    "\n",
    "        real_batch_sz = dance.shape[0]\n",
    "\n",
    "        # get initial states\n",
    "        # each batch is independent i.e. not a continuation of previous batch\n",
    "        # so we reset states for each batch\n",
    "        # POSSIBLE IMPROVEMENT: next batch is continuation of previous batch\n",
    "        g_states = model['g'].init_hidden(real_batch_sz)\n",
    "        d_state = model['d'].init_hidden(real_batch_sz)\n",
    "\n",
    "        #### GENERATOR ####\n",
    "        if not freeze_g:\n",
    "            optimizer['g'].zero_grad()\n",
    "        # prepare inputs\n",
    "        z = torch.empty([real_batch_sz, MAX_SEQ_LEN, num_feats]).uniform_() # random vector\n",
    "        dance = torch.Tensor(dance)\n",
    "\n",
    "        # feed inputs to generator\n",
    "        g_feats, _ = model['g'](z, g_states)\n",
    "\n",
    "        # calculate loss, backprop, and update weights of G\n",
    "        if isinstance(criterion['g'], GLoss):\n",
    "            d_logits_gen, _, _ = model['d'](g_feats, d_state)\n",
    "            loss['g'] = criterion['g'](d_logits_gen)\n",
    "        else: # feature matching\n",
    "            # feed real and generated input to discriminator\n",
    "            _, d_feats_real, _ = model['d'](dance, d_state)\n",
    "            _, d_feats_gen, _ = model['d'](g_feats, d_state)\n",
    "            loss['g'] = criterion['g'](d_feats_real, d_feats_gen)\n",
    "\n",
    "        if not freeze_g:\n",
    "            loss['g'].backward()\n",
    "            nn.utils.clip_grad_norm_(model['g'].parameters(), max_norm=MAX_GRAD_NORM)\n",
    "            optimizer['g'].step()\n",
    "\n",
    "        #### DISCRIMINATOR ####\n",
    "        if not freeze_d:\n",
    "            optimizer['d'].zero_grad()\n",
    "        # feed real and generated input to discriminator\n",
    "        d_logits_real, _, _ = model['d'](dance, d_state)\n",
    "        # need to detach from operation history to prevent backpropagating to generator\n",
    "        d_logits_gen, _, _ = model['d'](g_feats.detach(), d_state)\n",
    "        # calculate loss, backprop, and update weights of D\n",
    "        loss['d'] = criterion['d'](d_logits_real, d_logits_gen)\n",
    "        if not freeze_d:\n",
    "            loss['d'].backward()\n",
    "            nn.utils.clip_grad_norm_(model['d'].parameters(), max_norm=MAX_GRAD_NORM)\n",
    "            optimizer['d'].step()\n",
    "\n",
    "        g_loss_total += loss['g'].item()\n",
    "        d_loss_total += loss['d'].item()\n",
    "        num_corrects += (d_logits_real > 0.5).sum().item() + (d_logits_gen < 0.5).sum().item()\n",
    "        num_sample += real_batch_sz\n",
    "\n",
    "        # # fetch next batch\n",
    "        # batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='train')\n",
    "\n",
    "    g_loss_avg, d_loss_avg = 0.0, 0.0\n",
    "    d_acc = 0.0\n",
    "    if num_sample > 0:\n",
    "        g_loss_avg = g_loss_total / num_sample\n",
    "        d_loss_avg = d_loss_total / num_sample\n",
    "        d_acc = 100 * num_corrects / (2 * num_sample) # 2 because (real + generated)\n",
    "\n",
    "    return model, g_loss_avg, d_loss_avg, d_acc\n",
    "\n",
    "\n",
    "# def run_validation(model, criterion, dataloader):\n",
    "#     ''' Run single validation epoch\n",
    "#     '''\n",
    "#     num_feats = dataloader.get_num_body_parts()\n",
    "#     dataloader.rewind(part='validation')\n",
    "#     batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='validation')\n",
    "\n",
    "#     model['g'].eval()\n",
    "#     model['d'].eval()\n",
    "\n",
    "#     g_loss_total = 0.0\n",
    "#     d_loss_total = 0.0\n",
    "#     num_corrects = 0\n",
    "#     num_sample = 0\n",
    "\n",
    "#     while batch_meta is not None and batch_song is not None:\n",
    "\n",
    "#         real_batch_sz = batch_song.shape[0]\n",
    "\n",
    "#         # initial states\n",
    "#         g_states = model['g'].init_hidden(real_batch_sz)\n",
    "#         d_state = model['d'].init_hidden(real_batch_sz)\n",
    "\n",
    "#         #### GENERATOR ####\n",
    "#         # prepare inputs\n",
    "#         z = torch.empty([real_batch_sz, MAX_SEQ_LEN, num_feats]).uniform_() # random vector\n",
    "#         batch_song = torch.Tensor(batch_song)\n",
    "\n",
    "#         # feed inputs to generator\n",
    "#         g_feats, _ = model['g'](z, g_states)\n",
    "#         # feed real and generated input to discriminator\n",
    "#         d_logits_real, d_feats_real, _ = model['d'](batch_song, d_state)\n",
    "#         d_logits_gen, d_feats_gen, _ = model['d'](g_feats, d_state)\n",
    "#         # calculate loss\n",
    "#         if isinstance(criterion['g'], GLoss):\n",
    "#             g_loss = criterion['g'](d_logits_gen)\n",
    "#         else: # feature matching\n",
    "#             g_loss = criterion['g'](d_feats_real, d_feats_gen)\n",
    "\n",
    "#         d_loss = criterion['d'](d_logits_real, d_logits_gen)\n",
    "\n",
    "#         g_loss_total += g_loss.item()\n",
    "#         d_loss_total += d_loss.item()\n",
    "#         num_corrects += (d_logits_real > 0.5).sum().item() + (d_logits_gen < 0.5).sum().item()\n",
    "#         num_sample += real_batch_sz\n",
    "\n",
    "#         # fetch next batch\n",
    "#         batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='validation')\n",
    "\n",
    "#     g_loss_avg, d_loss_avg = 0.0, 0.0\n",
    "#     d_acc = 0.0\n",
    "#     if num_sample > 0:\n",
    "#         g_loss_avg = g_loss_total / num_sample\n",
    "#         d_loss_avg = d_loss_total / num_sample\n",
    "#         d_acc = 100 * num_corrects / (2 * num_sample) # 2 because (real + generated)\n",
    "\n",
    "#     return g_loss_avg, d_loss_avg, d_acc\n",
    "\n",
    "\n",
    "def run_epoch(model, optimizer, criterion, dataloader, ep, num_ep,\n",
    "              freeze_g=False, freeze_d=False, pretraining=False):\n",
    "    ''' Run a single epoch\n",
    "    '''\n",
    "    model, trn_g_loss, trn_d_loss, trn_acc = \\\n",
    "        run_training(model, optimizer, criterion, dataloader, freeze_g=freeze_g, freeze_d=freeze_d)\n",
    "\n",
    "    # val_g_loss, val_d_loss, val_acc = run_validation(model, criterion, dataloader)\n",
    "\n",
    "    if pretraining:\n",
    "        print(\"Pretraining Epoch %d/%d \" % (ep+1, num_ep), \"[Freeze G: \", freeze_g, \", Freeze D: \", freeze_d, \"]\")\n",
    "    else:\n",
    "        print(\"Epoch %d/%d \" % (ep+1, num_ep), \"[Freeze G: \", freeze_g, \", Freeze D: \", freeze_d, \"]\")\n",
    "        print(\"\\t[Training] G_loss: %0.8f, D_loss: %0.8f, D_acc: %0.2f\" % (trn_g_loss, trn_d_loss, trn_acc))\n",
    "#     print(\"\\t[Training] G_loss: %0.8f, D_loss: %0.8f, D_acc: %0.2f\\n\"\n",
    "#           \"\\t[Validation] G_loss: %0.8f, D_loss: %0.8f, D_acc: %0.2f\" %\n",
    "#           (trn_g_loss, trn_d_loss, trn_acc)\n",
    "#         #    val_g_loss, val_d_loss, val_acc)\n",
    "# \t\t   )\n",
    "# FIX\n",
    "        \n",
    "\n",
    "    # -- DEBUG --\n",
    "    # This is for monitoring the current output from generator\n",
    "    # generate from model then save to MIDI file\n",
    "    g_states = model['g'].init_hidden(1)\n",
    "    num_feats = train_dataset.get_num_body_parts()\n",
    "    z = torch.empty([1, MAX_SEQ_LEN, num_feats]).uniform_() # random vector\n",
    "    if torch.cuda.is_available():\n",
    "        z = z.cuda()\n",
    "        model['g'].cuda()\n",
    "\n",
    "    model['g'].eval()\n",
    "    g_feats, _ = model['g'](z, g_states)\n",
    "    dance_data = g_feats.squeeze().cpu()\n",
    "    dance_data = dance_data.detach().numpy()\n",
    "\n",
    "    # FIX - this is p bad\n",
    "    if (ep+1) == num_ep:\n",
    "        generated_dance = train_dataset.save_data('sample{}_final.dance'.format(num_ep), dance_data)\n",
    "    else:\n",
    "        generated_dance = train_dataset.save_data('sample{}.dance'.format(num_ep), dance_data)\n",
    "    # -- DEBUG --\n",
    "\n",
    "    return model, trn_acc\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    ''' Training sequence\n",
    "    '''\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "    dataloader = train_loader\n",
    "    num_feats = train_dataset.get_num_body_parts() # FIX\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    if train_on_gpu:\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    "\n",
    "    model = {\n",
    "        'g': Generator(num_feats, use_cuda=train_on_gpu),\n",
    "        'd': Discriminator(num_feats, use_cuda=train_on_gpu)\n",
    "    }\n",
    "\n",
    "    if args.use_sgd:\n",
    "        optimizer = {\n",
    "            'g': optim.SGD(model['g'].parameters(), lr=args.g_lrn_rate, momentum=0.9),\n",
    "            'd': optim.SGD(model['d'].parameters(), lr=args.d_lrn_rate, momentum=0.9)\n",
    "        }\n",
    "    else:\n",
    "        optimizer = {\n",
    "            'g': optim.Adam(model['g'].parameters(), args.g_lrn_rate),\n",
    "            'd': optim.Adam(model['d'].parameters(), args.d_lrn_rate)\n",
    "        }\n",
    "\n",
    "    criterion = {\n",
    "        'g': nn.MSELoss(reduction='sum') if args.feature_matching else GLoss(),\n",
    "        'd': DLoss(args.label_smoothing)\n",
    "    }\n",
    "\n",
    "    if args.load_g:\n",
    "        ckpt = torch.load(os.path.join(CKPT_DIR, G_FN))\n",
    "        model['g'].load_state_dict(ckpt)\n",
    "        print(\"Continue training of %s\" % os.path.join(CKPT_DIR, G_FN))\n",
    "\n",
    "    if args.load_d:\n",
    "        ckpt = torch.load(os.path.join(CKPT_DIR, D_FN))\n",
    "        model['d'].load_state_dict(ckpt)\n",
    "        print(\"Continue training of %s\" % os.path.join(CKPT_DIR, D_FN))\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model['g'].cuda()\n",
    "        model['d'].cuda()\n",
    "\n",
    "    if not args.no_pretraining:\n",
    "        for ep in range(args.d_pretraining_epochs):\n",
    "            model, _ = run_epoch(model, optimizer, criterion, dataloader,\n",
    "                              ep, args.d_pretraining_epochs, freeze_g=True, pretraining=True)\n",
    "\n",
    "        for ep in range(args.g_pretraining_epochs):\n",
    "            model, _ = run_epoch(model, optimizer, criterion, dataloader,\n",
    "                              ep, args.g_pretraining_epochs, freeze_d=True, pretraining=True)\n",
    "\n",
    "    freeze_d = False\n",
    "    for ep in range(args.num_epochs):\n",
    "        # if ep % args.freeze_d_every == 0:\n",
    "        #     freeze_d = not freeze_d\n",
    "\n",
    "        model, trn_acc = run_epoch(model, optimizer, criterion, dataloader, ep, args.num_epochs, freeze_d=freeze_d)\n",
    "        if args.conditional_freezing:\n",
    "            # conditional freezing\n",
    "            freeze_d = False\n",
    "            if trn_acc >= 95.0:\n",
    "                freeze_d = True\n",
    "\n",
    "    if not args.no_save_g:\n",
    "        torch.save(model['g'].state_dict(), os.path.join(CKPT_DIR, G_FN))\n",
    "        print(\"Saved generator: %s\" % os.path.join(CKPT_DIR, G_FN))\n",
    "\n",
    "    if not args.no_save_d:\n",
    "        torch.save(model['d'].state_dict(), os.path.join(CKPT_DIR, D_FN))\n",
    "        print(\"Saved discriminator: %s\" % os.path.join(CKPT_DIR, D_FN))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "# ARG_PARSER = ArgumentParser()\n",
    "# ARG_PARSER.add_argument('--load_g', action='store_true')\n",
    "# ARG_PARSER.add_argument('--load_d', action='store_true')\n",
    "# ARG_PARSER.add_argument('--no_save_g', action='store_true')\n",
    "# ARG_PARSER.add_argument('--no_save_d', action='store_true')\n",
    "\n",
    "# ARG_PARSER.add_argument('--num_epochs', default=300, type=int)\n",
    "# ARG_PARSER.add_argument('--batch_size', default=16, type=int)\n",
    "# ARG_PARSER.add_argument('--g_lrn_rate', default=0.001, type=float)\n",
    "# ARG_PARSER.add_argument('--d_lrn_rate', default=0.001, type=float)\n",
    "\n",
    "# ARG_PARSER.add_argument('--no_pretraining', action='store_true')\n",
    "# ARG_PARSER.add_argument('--g_pretraining_epochs', default=5, type=int)\n",
    "# ARG_PARSER.add_argument('--d_pretraining_epochs', default=5, type=int)\n",
    "# ARG_PARSER.add_argument('--use_sgd', action='store_true')\n",
    "# ARG_PARSER.add_argument('--conditional_freezing', action='store_true')\n",
    "# ARG_PARSER.add_argument('--label_smoothing', action='store_true')\n",
    "# ARG_PARSER.add_argument('--feature_matching', action='store_true')\n",
    "\n",
    "class ARGS():\n",
    "    def __init__(self):\n",
    "        self.load_g =  False\n",
    "        self.load_d =  False\n",
    "        self.no_save_g =  False\n",
    "        self.no_save_d =  False\n",
    "\n",
    "        self.num_epochs =  500\n",
    "        self.batch_size =  8\n",
    "        self.g_lrn_rate =  0.0000001\n",
    "        self.d_lrn_rate =  0.0000001\n",
    "\n",
    "        self.no_pretraining =  False\n",
    "        self.g_pretraining_epochs =  5\n",
    "        self.d_pretraining_epochs =  5\n",
    "        self.use_sgd =  False\n",
    "        self.conditional_freezing =  False\n",
    "        self.label_smoothing =  False\n",
    "        self.feature_matching =  False\n",
    "ARGS = ARGS()\n",
    "# ARGS = ARG_PARSER.parse_args()\n",
    "MAX_SEQ_LEN = TOTAL_FRAMES # todo\n",
    "BATCH_SIZE = ARGS.batch_size\n",
    "\n",
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
