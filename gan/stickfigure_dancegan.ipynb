{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:00<00:00, 13.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:00<00:00, 13.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:00<00:00, 13.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json, codecs\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We preprocess'd the file names so we have a index with a list of all frames per dance id\n",
    "FRAME_LIST_INDEX = './dance-frame-list.json'\n",
    "\n",
    "# np.random.seed(0)\n",
    "NUM_BODY_PARTS = 13\n",
    "TOTAL_FRAMES = 250\n",
    "\n",
    "# We have 250 frames. We are going to going to take the 17 body parts, \n",
    "# and turn it into 13 (remove eyes and ears). Then 13x2 (13 body parts, 2 vectors)\n",
    "def from_motion_to_numpy_vector(motion):\n",
    "    # For now, we only take the first person. Later we can maybe try to feed in all people, or do batches of two\n",
    "    motion_vector = np.zeros((250, NUM_BODY_PARTS, 2))\n",
    "    for i, frame in enumerate(motion):\n",
    "        if len(frame) > 0 and i < TOTAL_FRAMES:\n",
    "            current_frame_data = frame\n",
    "            # TODO extend this past just 1 person\n",
    "            person0 = current_frame_data[0][1:]\n",
    "            current_frame_vector = np.zeros((NUM_BODY_PARTS, 2))\n",
    "            current_body_part_idx = 0\n",
    "            for body_part_data in person0:\n",
    "                body_part = body_part_data[0]\n",
    "                if body_part not in ['left_eye', 'left_ear', 'right_eye', 'right_ear']:\n",
    "                    current_frame_vector[current_body_part_idx] = body_part_data[1]\n",
    "                    current_body_part_idx = current_body_part_idx + 1\n",
    "            motion_vector[i] = current_frame_vector\n",
    "    return motion_vector\n",
    "\n",
    "def from_numpy_vector_to_motion_coordinates(motion_vector):\n",
    "    # Reshape so each element in array is an a NUM_BODY_PARTS x 2 array that has coordinates\n",
    "    return motion_vector.reshape(TOTAL_FRAMES, NUM_BODY_PARTS, 2)\n",
    "\n",
    "class LetsDanceDataset(torch.utils.data.Dataset):\n",
    "    categories_hash = {'tango': 0, 'break': 1, 'swing': 2,'quickstep': 3,\n",
    "                  'foxtrot': 4,'pasodoble': 5,'tap': 6,'samba': 7,'flamenco': 8,\n",
    "                  'ballet': 9,'rumba': 10,'waltz': 11,'cha': 12,'latin': 13,\n",
    "                  'square': 14,'jive': 15}\n",
    "    \n",
    "    # Precomputed\n",
    "    MEAN=torch.Tensor([[722.8463, 230.9753], [725.5026, 284.8430], [718.0136, 283.9306], [729.7226, 332.3776], \n",
    "          [717.4737, 331.9450], [731.9489, 333.1949], [719.6969, 335.0007], [724.7956, 446.3675],\n",
    "          [719.7034, 446.8887], [727.5336, 563.0570], [720.0659, 563.5020], [729.5637, 658.3285],\n",
    "          [716.7125, 658.4642]])\n",
    "    \n",
    "    STD = torch.Tensor([[248.5471,  54.5708], [253.7432,  50.6963], [256.4125,  50.9480], [259.8698,  64.7350],\n",
    "           [262.0792,  64.8512], [262.9285,  85.9804], [260.5722,  86.1529], [254.4909,  51.1837],\n",
    "           [256.7613,  51.5563], [253.6787,  62.7815], [256.8294,  62.8685], [260.5059,  70.0873],\n",
    "           [262.8192,  68.4242]])\n",
    "    \n",
    "    def __init__(self, root_dir, dances):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.data = np.zeros((len(dances), TOTAL_FRAMES, NUM_BODY_PARTS, 2))\n",
    "        self.metadata = dances\n",
    "        \n",
    "        dances = list(filter(lambda dance: dance[2] >= TOTAL_FRAMES, dances))\n",
    "        \n",
    "        for i, dance in enumerate(tqdm(dances)):\n",
    "            [category, dance_id, frames] = dance\n",
    "            current_frame_path = \"{}{}/{}.json\".format(root_dir, category, dance_id)\n",
    "            with open(current_frame_path) as f:\n",
    "                motion = json.load(f)\n",
    "            self.data[i] = from_motion_to_numpy_vector(motion)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize(mean=torch.Tensor(self.MEAN),\n",
    "                               std=torch.Tensor(self.STD)),\n",
    "        ])\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def getitem_metadata(self, index):\n",
    "        return self.metadata[index]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns (category, motion)\n",
    "        motion is in shape of `(NUM_FRAMES, 13, 2)`\n",
    "        data is normalized \n",
    "        '''\n",
    "        # todo add transform\n",
    "        data = torch.Tensor(self.data[index])\n",
    "        data = self.normalize(data)\n",
    "        return data\n",
    "\n",
    "    def get_num_body_parts(self):\n",
    "        return NUM_BODY_PARTS\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_data(self, filename, np_array):\n",
    "        # TODO \n",
    "        file_path = 'data/' + filename + \".json\"\n",
    "        d = np_array.tolist()\n",
    "#         d = from_numpy_vector_to_motion_coordinates(np_array).tolist()\n",
    "        json.dump(d, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True)\n",
    "    \n",
    "\n",
    "    def normalize(self, motion):\n",
    "        return (motion - self.MEAN) / (self.STD)\n",
    "    \n",
    "    def denormalize(self, motion):\n",
    "        return (motion * self.STD) + self.MEAN\n",
    "\n",
    "# For this first test, we are just using Latin dances\n",
    "with open(FRAME_LIST_INDEX) as f:\n",
    "    frames_index = json.load(f)\n",
    "    np.random.shuffle(frames_index)\n",
    "\n",
    "    \n",
    "# train_dances= frames_index[:1000]\n",
    "# valid_dances = frames_index[100:]\n",
    "# train_dataset = LetsDanceDataset('../densepose/full/', train_dances)\n",
    "# valid_dataset = LetsDanceDataset('../densepose/full/', valid_dances)\n",
    "\n",
    "mini_dataset = LetsDanceDataset('../densepose/full/', frames_index[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0835, -0.7903,  0.0752,  ...,  5.9665, -0.0803,  6.1095],\n",
       "        [ 0.0732, -0.7499,  0.0573,  ...,  5.9665,  1.0394,  6.1095],\n",
       "        [ 0.0872, -0.5644,  0.0632,  ...,  5.9665,  1.0301,  6.1095],\n",
       "        ...,\n",
       "        [ 1.4977, -2.1652,  2.3629,  ...,  4.3849,  0.7016,  4.4895],\n",
       "        [ 1.7235, -1.0195,  2.5406,  ...,  4.9546,  0.7213,  4.1823],\n",
       "        [ 1.8532, -1.6082,  1.9623,  ...,  3.8469,  0.8097,  3.9531]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset[9].reshape(250, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 2/26 [00:00<00:02,  9.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 3/26 [00:00<00:02,  7.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 4/26 [00:00<00:03,  6.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 7/26 [00:00<00:02,  9.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 10/26 [00:00<00:01, 11.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▌     | 12/26 [00:01<00:01, 10.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 14/26 [00:01<00:01, 10.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 16/26 [00:01<00:00, 10.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 18/26 [00:01<00:00, 10.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 20/26 [00:01<00:00, 10.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▍ | 22/26 [00:02<00:00, 10.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 24/26 [00:02<00:00, 11.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 26/26 [00:02<00:00, 10.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[722.8463, 230.9753],\n",
       "        [725.5026, 284.8430],\n",
       "        [718.0136, 283.9306],\n",
       "        [729.7226, 332.3776],\n",
       "        [717.4737, 331.9450],\n",
       "        [731.9489, 333.1949],\n",
       "        [719.6969, 335.0007],\n",
       "        [724.7956, 446.3675],\n",
       "        [719.7034, 446.8887],\n",
       "        [727.5336, 563.0570],\n",
       "        [720.0659, 563.5020],\n",
       "        [729.5637, 658.3285],\n",
       "        [716.7125, 658.4642]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset = frames_index[:30]\n",
    "test_dataset = LetsDanceDataset('../densepose/full/', small_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.],\n",
       "         [ 2.,  3.],\n",
       "         [ 4.,  5.],\n",
       "         [ 6.,  7.]],\n",
       "\n",
       "        [[ 8.,  9.],\n",
       "         [10., 11.],\n",
       "         [12., 13.],\n",
       "         [14., 15.]],\n",
       "\n",
       "        [[16., 17.],\n",
       "         [18., 19.],\n",
       "         [20., 21.],\n",
       "         [22., 23.]],\n",
       "\n",
       "        [[24., 25.],\n",
       "         [26., 27.],\n",
       "         [28., 29.],\n",
       "         [30., 31.]],\n",
       "\n",
       "        [[32., 33.],\n",
       "         [34., 35.],\n",
       "         [36., 37.],\n",
       "         [38., 39.]],\n",
       "\n",
       "        [[40., 41.],\n",
       "         [42., 43.],\n",
       "         [44., 45.],\n",
       "         [46., 47.]],\n",
       "\n",
       "        [[48., 49.],\n",
       "         [50., 51.],\n",
       "         [52., 53.],\n",
       "         [54., 55.]],\n",
       "\n",
       "        [[56., 57.],\n",
       "         [58., 59.],\n",
       "         [60., 61.],\n",
       "         [62., 63.]],\n",
       "\n",
       "        [[64., 65.],\n",
       "         [66., 67.],\n",
       "         [68., 69.],\n",
       "         [70., 71.]],\n",
       "\n",
       "        [[72., 73.],\n",
       "         [74., 75.],\n",
       "         [76., 77.],\n",
       "         [78., 79.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.arange(80).reshape(10, 4, 2).double()\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000],\n",
       "         [ 2.0000,  3.0000],\n",
       "         [ 4.0000,  5.0000],\n",
       "         [ 6.0000,  7.0000]],\n",
       "\n",
       "        [[ 8.0000,  9.0000],\n",
       "         [10.0000, 11.0000],\n",
       "         [12.0000, 13.0000],\n",
       "         [14.0000, 15.0000]],\n",
       "\n",
       "        [[16.0000, 17.0000],\n",
       "         [18.0000, 19.0000],\n",
       "         [20.0000, 21.0000],\n",
       "         [22.0000, 23.0000]],\n",
       "\n",
       "        [[24.0000, 25.0000],\n",
       "         [26.0000, 27.0000],\n",
       "         [28.0000, 29.0000],\n",
       "         [30.0000, 31.0000]],\n",
       "\n",
       "        [[32.0000, 33.0000],\n",
       "         [34.0000, 35.0000],\n",
       "         [36.0000, 37.0000],\n",
       "         [38.0000, 39.0000]],\n",
       "\n",
       "        [[40.0000, 41.0000],\n",
       "         [42.0000, 43.0000],\n",
       "         [44.0000, 45.0000],\n",
       "         [46.0000, 47.0000]],\n",
       "\n",
       "        [[48.0000, 49.0000],\n",
       "         [50.0000, 51.0000],\n",
       "         [52.0000, 53.0000],\n",
       "         [54.0000, 55.0000]],\n",
       "\n",
       "        [[56.0000, 57.0000],\n",
       "         [58.0000, 59.0000],\n",
       "         [60.0000, 61.0000],\n",
       "         [62.0000, 63.0000]],\n",
       "\n",
       "        [[64.0000, 65.0000],\n",
       "         [66.0000, 67.0000],\n",
       "         [68.0000, 69.0000],\n",
       "         [70.0000, 71.0000]],\n",
       "\n",
       "        [[72.0000, 73.0000],\n",
       "         [74.0000, 75.0000],\n",
       "         [76.0000, 77.0000],\n",
       "         [78.0000, 79.0000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of denorm and norm\n",
    "mean = v.mean(0)\n",
    "std = v.std(0)\n",
    "mean /= len(v)\n",
    "std /= len(v)\n",
    "\n",
    "def normalize(v, mean, std):\n",
    "    return (v - mean)/std\n",
    "def denormalize(v, mean, std):\n",
    "    return (v * std) + mean\n",
    "\n",
    "denormalize(normalize(v, mean, std), mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from c_rnn_gan import Generator, Discriminator\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "CKPT_DIR = 'models'\n",
    "\n",
    "G_FN = 'c_rnn_gan_g.pth'\n",
    "D_FN = 'c_rnn_gan_d.pth'\n",
    "\n",
    "G_LRN_RATE = 0.001\n",
    "D_LRN_RATE = 0.001\n",
    "MAX_GRAD_NORM = 5.0\n",
    "# following values are modified at runtime\n",
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "EPSILON = 1e-40 # value to use to approximate zero (to prevent undefined results)\n",
    "\n",
    "class GLoss(nn.Module):\n",
    "    ''' C-RNN-GAN generator loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(GLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits_gen):\n",
    "        logits_gen = torch.clamp(logits_gen, EPSILON, 1.0)\n",
    "        batch_loss = -torch.log(logits_gen)\n",
    "\n",
    "        return torch.mean(batch_loss)\n",
    "\n",
    "\n",
    "class DLoss(nn.Module):\n",
    "    ''' C-RNN-GAN discriminator loss\n",
    "    '''\n",
    "    def __init__(self, label_smoothing=False):\n",
    "        super(DLoss, self).__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, logits_real, logits_gen):\n",
    "        ''' Discriminator loss\n",
    "\n",
    "        logits_real: logits from D, when input is real\n",
    "        logits_gen: logits from D, when input is from Generator\n",
    "\n",
    "        loss = -(ylog(p) + (1-y)log(1-p))\n",
    "\n",
    "        '''\n",
    "        logits_real = torch.clamp(logits_real, EPSILON, 1.0)\n",
    "        d_loss_real = -torch.log(logits_real)\n",
    "\n",
    "        if self.label_smoothing:\n",
    "            p_fake = torch.clamp((1 - logits_real), EPSILON, 1.0)\n",
    "            d_loss_fake = -torch.log(p_fake)\n",
    "            d_loss_real = 0.9*d_loss_real + 0.1*d_loss_fake\n",
    "\n",
    "        logits_gen = torch.clamp((1 - logits_gen), EPSILON, 1.0)\n",
    "        d_loss_gen = -torch.log(logits_gen)\n",
    "\n",
    "        batch_loss = d_loss_real + d_loss_gen\n",
    "        return torch.mean(batch_loss)\n",
    "\n",
    "\n",
    "def run_training(model, optimizer, criterion, dataloader, freeze_g=False, freeze_d=False):\n",
    "    ''' Run single training epoch\n",
    "    '''\n",
    "    \n",
    "    num_feats = train_dataset.get_num_body_parts()\n",
    "    # dataloader.rewind(part='train')\n",
    "    # batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='train')\n",
    "\n",
    "    model['g'].train()\n",
    "    model['d'].train()\n",
    "\n",
    "    loss = {}\n",
    "    g_loss_total = 0.0\n",
    "    d_loss_total = 0.0\n",
    "    num_corrects = 0\n",
    "    num_sample = 0\n",
    "\n",
    "    for step, dance in enumerate(dataloader):\n",
    "\n",
    "        real_batch_sz = dance.shape[0]\n",
    "\n",
    "        # get initial states\n",
    "        # each batch is independent i.e. not a continuation of previous batch\n",
    "        # so we reset states for each batch\n",
    "        # POSSIBLE IMPROVEMENT: next batch is continuation of previous batch\n",
    "        g_states = model['g'].init_hidden(real_batch_sz)\n",
    "        d_state = model['d'].init_hidden(real_batch_sz)\n",
    "\n",
    "        #### GENERATOR ####\n",
    "        if not freeze_g:\n",
    "            optimizer['g'].zero_grad()\n",
    "        # prepare inputs\n",
    "        z = torch.empty([real_batch_sz, MAX_SEQ_LEN, num_feats]).uniform_() # random vector\n",
    "        dance = torch.Tensor(dance)\n",
    "\n",
    "        # feed inputs to generator\n",
    "        g_feats, _ = model['g'](z, g_states)\n",
    "\n",
    "        # calculate loss, backprop, and update weights of G\n",
    "        if isinstance(criterion['g'], GLoss):\n",
    "            d_logits_gen, _, _ = model['d'](g_feats, d_state)\n",
    "            loss['g'] = criterion['g'](d_logits_gen)\n",
    "        else: # feature matching\n",
    "            # feed real and generated input to discriminator\n",
    "            _, d_feats_real, _ = model['d'](dance, d_state)\n",
    "            _, d_feats_gen, _ = model['d'](g_feats, d_state)\n",
    "            loss['g'] = criterion['g'](d_feats_real, d_feats_gen)\n",
    "\n",
    "        if not freeze_g:\n",
    "            loss['g'].backward()\n",
    "            nn.utils.clip_grad_norm_(model['g'].parameters(), max_norm=MAX_GRAD_NORM)\n",
    "            optimizer['g'].step()\n",
    "\n",
    "        #### DISCRIMINATOR ####\n",
    "        if not freeze_d:\n",
    "            optimizer['d'].zero_grad()\n",
    "        # feed real and generated input to discriminator\n",
    "        d_logits_real, _, _ = model['d'](dance, d_state)\n",
    "        # need to detach from operation history to prevent backpropagating to generator\n",
    "        d_logits_gen, _, _ = model['d'](g_feats.detach(), d_state)\n",
    "        # calculate loss, backprop, and update weights of D\n",
    "        loss['d'] = criterion['d'](d_logits_real, d_logits_gen)\n",
    "        if not freeze_d:\n",
    "            loss['d'].backward()\n",
    "            nn.utils.clip_grad_norm_(model['d'].parameters(), max_norm=MAX_GRAD_NORM)\n",
    "            optimizer['d'].step()\n",
    "\n",
    "        g_loss_total += loss['g'].item()\n",
    "        d_loss_total += loss['d'].item()\n",
    "        num_corrects += (d_logits_real > 0.5).sum().item() + (d_logits_gen < 0.5).sum().item()\n",
    "        num_sample += real_batch_sz\n",
    "\n",
    "        # # fetch next batch\n",
    "        # batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='train')\n",
    "\n",
    "    g_loss_avg, d_loss_avg = 0.0, 0.0\n",
    "    d_acc = 0.0\n",
    "    if num_sample > 0:\n",
    "        g_loss_avg = g_loss_total / num_sample\n",
    "        d_loss_avg = d_loss_total / num_sample\n",
    "        d_acc = 100 * num_corrects / (2 * num_sample) # 2 because (real + generated)\n",
    "\n",
    "    return model, g_loss_avg, d_loss_avg, d_acc\n",
    "\n",
    "\n",
    "# def run_validation(model, criterion, dataloader):\n",
    "#     ''' Run single validation epoch\n",
    "#     '''\n",
    "#     num_feats = dataloader.get_num_body_parts()\n",
    "#     dataloader.rewind(part='validation')\n",
    "#     batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='validation')\n",
    "\n",
    "#     model['g'].eval()\n",
    "#     model['d'].eval()\n",
    "\n",
    "#     g_loss_total = 0.0\n",
    "#     d_loss_total = 0.0\n",
    "#     num_corrects = 0\n",
    "#     num_sample = 0\n",
    "\n",
    "#     while batch_meta is not None and batch_song is not None:\n",
    "\n",
    "#         real_batch_sz = batch_song.shape[0]\n",
    "\n",
    "#         # initial states\n",
    "#         g_states = model['g'].init_hidden(real_batch_sz)\n",
    "#         d_state = model['d'].init_hidden(real_batch_sz)\n",
    "\n",
    "#         #### GENERATOR ####\n",
    "#         # prepare inputs\n",
    "#         z = torch.empty([real_batch_sz, MAX_SEQ_LEN, num_feats]).uniform_() # random vector\n",
    "#         batch_song = torch.Tensor(batch_song)\n",
    "\n",
    "#         # feed inputs to generator\n",
    "#         g_feats, _ = model['g'](z, g_states)\n",
    "#         # feed real and generated input to discriminator\n",
    "#         d_logits_real, d_feats_real, _ = model['d'](batch_song, d_state)\n",
    "#         d_logits_gen, d_feats_gen, _ = model['d'](g_feats, d_state)\n",
    "#         # calculate loss\n",
    "#         if isinstance(criterion['g'], GLoss):\n",
    "#             g_loss = criterion['g'](d_logits_gen)\n",
    "#         else: # feature matching\n",
    "#             g_loss = criterion['g'](d_feats_real, d_feats_gen)\n",
    "\n",
    "#         d_loss = criterion['d'](d_logits_real, d_logits_gen)\n",
    "\n",
    "#         g_loss_total += g_loss.item()\n",
    "#         d_loss_total += d_loss.item()\n",
    "#         num_corrects += (d_logits_real > 0.5).sum().item() + (d_logits_gen < 0.5).sum().item()\n",
    "#         num_sample += real_batch_sz\n",
    "\n",
    "#         # fetch next batch\n",
    "#         batch_meta, batch_song = dataloader.get_batch(BATCH_SIZE, MAX_SEQ_LEN, part='validation')\n",
    "\n",
    "#     g_loss_avg, d_loss_avg = 0.0, 0.0\n",
    "#     d_acc = 0.0\n",
    "#     if num_sample > 0:\n",
    "#         g_loss_avg = g_loss_total / num_sample\n",
    "#         d_loss_avg = d_loss_total / num_sample\n",
    "#         d_acc = 100 * num_corrects / (2 * num_sample) # 2 because (real + generated)\n",
    "\n",
    "#     return g_loss_avg, d_loss_avg, d_acc\n",
    "\n",
    "\n",
    "def run_epoch(model, optimizer, criterion, dataloader, ep, num_ep,\n",
    "              freeze_g=False, freeze_d=False, pretraining=False):\n",
    "    ''' Run a single epoch\n",
    "    '''\n",
    "    model, trn_g_loss, trn_d_loss, trn_acc = \\\n",
    "        run_training(model, optimizer, criterion, dataloader, freeze_g=freeze_g, freeze_d=freeze_d)\n",
    "\n",
    "    # val_g_loss, val_d_loss, val_acc = run_validation(model, criterion, dataloader)\n",
    "\n",
    "    if pretraining:\n",
    "        print(\"Pretraining Epoch %d/%d \" % (ep+1, num_ep), \"[Freeze G: \", freeze_g, \", Freeze D: \", freeze_d, \"]\")\n",
    "    else:\n",
    "        print(\"Epoch %d/%d \" % (ep+1, num_ep), \"[Freeze G: \", freeze_g, \", Freeze D: \", freeze_d, \"]\")\n",
    "        print(\"\\t[Training] G_loss: %0.8f, D_loss: %0.8f, D_acc: %0.2f\" % (trn_g_loss, trn_d_loss, trn_acc))\n",
    "#     print(\"\\t[Training] G_loss: %0.8f, D_loss: %0.8f, D_acc: %0.2f\\n\"\n",
    "#           \"\\t[Validation] G_loss: %0.8f, D_loss: %0.8f, D_acc: %0.2f\" %\n",
    "#           (trn_g_loss, trn_d_loss, trn_acc)\n",
    "#         #    val_g_loss, val_d_loss, val_acc)\n",
    "# \t\t   )\n",
    "# FIX\n",
    "        \n",
    "\n",
    "    # -- DEBUG --\n",
    "    # This is for monitoring the current output from generator\n",
    "    # generate from model then save to MIDI file\n",
    "    g_states = model['g'].init_hidden(1)\n",
    "    num_feats = train_dataset.get_num_body_parts()\n",
    "    z = torch.empty([1, MAX_SEQ_LEN, num_feats]).uniform_() # random vector\n",
    "    if torch.cuda.is_available():\n",
    "        z = z.cuda()\n",
    "        model['g'].cuda()\n",
    "\n",
    "    model['g'].eval()\n",
    "    g_feats, _ = model['g'](z, g_states)\n",
    "    dance_data = g_feats.squeeze().cpu()\n",
    "    dance_data = dance_data.detach().numpy()\n",
    "\n",
    "    # FIX - this is p bad\n",
    "    if (ep+1) == num_ep:\n",
    "        generated_dance = train_dataset.save_data('sample{}_final.dance'.format(num_ep), dance_data)\n",
    "    else:\n",
    "        generated_dance = train_dataset.save_data('sample{}.dance'.format(num_ep), dance_data)\n",
    "    # -- DEBUG --\n",
    "\n",
    "    return model, trn_acc\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    ''' Training sequence\n",
    "    '''\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "    dataloader = train_loader\n",
    "    num_feats = train_dataset.get_num_body_parts() # FIX\n",
    "\n",
    "    # First checking if GPU is available\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    if train_on_gpu:\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    "\n",
    "    model = {\n",
    "        'g': Generator(num_feats, use_cuda=train_on_gpu),\n",
    "        'd': Discriminator(num_feats, use_cuda=train_on_gpu)\n",
    "    }\n",
    "\n",
    "    if args.use_sgd:\n",
    "        optimizer = {\n",
    "            'g': optim.SGD(model['g'].parameters(), lr=args.g_lrn_rate, momentum=0.9),\n",
    "            'd': optim.SGD(model['d'].parameters(), lr=args.d_lrn_rate, momentum=0.9)\n",
    "        }\n",
    "    else:\n",
    "        optimizer = {\n",
    "            'g': optim.Adam(model['g'].parameters(), args.g_lrn_rate),\n",
    "            'd': optim.Adam(model['d'].parameters(), args.d_lrn_rate)\n",
    "        }\n",
    "\n",
    "    criterion = {\n",
    "        'g': nn.MSELoss(reduction='sum') if args.feature_matching else GLoss(),\n",
    "        'd': DLoss(args.label_smoothing)\n",
    "    }\n",
    "\n",
    "    if args.load_g:\n",
    "        ckpt = torch.load(os.path.join(CKPT_DIR, G_FN))\n",
    "        model['g'].load_state_dict(ckpt)\n",
    "        print(\"Continue training of %s\" % os.path.join(CKPT_DIR, G_FN))\n",
    "\n",
    "    if args.load_d:\n",
    "        ckpt = torch.load(os.path.join(CKPT_DIR, D_FN))\n",
    "        model['d'].load_state_dict(ckpt)\n",
    "        print(\"Continue training of %s\" % os.path.join(CKPT_DIR, D_FN))\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model['g'].cuda()\n",
    "        model['d'].cuda()\n",
    "\n",
    "    if not args.no_pretraining:\n",
    "        for ep in range(args.d_pretraining_epochs):\n",
    "            model, _ = run_epoch(model, optimizer, criterion, dataloader,\n",
    "                              ep, args.d_pretraining_epochs, freeze_g=True, pretraining=True)\n",
    "\n",
    "        for ep in range(args.g_pretraining_epochs):\n",
    "            model, _ = run_epoch(model, optimizer, criterion, dataloader,\n",
    "                              ep, args.g_pretraining_epochs, freeze_d=True, pretraining=True)\n",
    "\n",
    "    freeze_d = False\n",
    "    for ep in range(args.num_epochs):\n",
    "        # if ep % args.freeze_d_every == 0:\n",
    "        #     freeze_d = not freeze_d\n",
    "\n",
    "        model, trn_acc = run_epoch(model, optimizer, criterion, dataloader, ep, args.num_epochs, freeze_d=freeze_d)\n",
    "        if args.conditional_freezing:\n",
    "            # conditional freezing\n",
    "            freeze_d = False\n",
    "            if trn_acc >= 95.0:\n",
    "                freeze_d = True\n",
    "\n",
    "    if not args.no_save_g:\n",
    "        torch.save(model['g'].state_dict(), os.path.join(CKPT_DIR, G_FN))\n",
    "        print(\"Saved generator: %s\" % os.path.join(CKPT_DIR, G_FN))\n",
    "\n",
    "    if not args.no_save_d:\n",
    "        torch.save(model['d'].state_dict(), os.path.join(CKPT_DIR, D_FN))\n",
    "        print(\"Saved discriminator: %s\" % os.path.join(CKPT_DIR, D_FN))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "# ARG_PARSER = ArgumentParser()\n",
    "# ARG_PARSER.add_argument('--load_g', action='store_true')\n",
    "# ARG_PARSER.add_argument('--load_d', action='store_true')\n",
    "# ARG_PARSER.add_argument('--no_save_g', action='store_true')\n",
    "# ARG_PARSER.add_argument('--no_save_d', action='store_true')\n",
    "\n",
    "# ARG_PARSER.add_argument('--num_epochs', default=300, type=int)\n",
    "# ARG_PARSER.add_argument('--batch_size', default=16, type=int)\n",
    "# ARG_PARSER.add_argument('--g_lrn_rate', default=0.001, type=float)\n",
    "# ARG_PARSER.add_argument('--d_lrn_rate', default=0.001, type=float)\n",
    "\n",
    "# ARG_PARSER.add_argument('--no_pretraining', action='store_true')\n",
    "# ARG_PARSER.add_argument('--g_pretraining_epochs', default=5, type=int)\n",
    "# ARG_PARSER.add_argument('--d_pretraining_epochs', default=5, type=int)\n",
    "# ARG_PARSER.add_argument('--use_sgd', action='store_true')\n",
    "# ARG_PARSER.add_argument('--conditional_freezing', action='store_true')\n",
    "# ARG_PARSER.add_argument('--label_smoothing', action='store_true')\n",
    "# ARG_PARSER.add_argument('--feature_matching', action='store_true')\n",
    "\n",
    "class ARGS():\n",
    "    def __init__(self):\n",
    "        self.load_g =  False\n",
    "        self.load_d =  False\n",
    "        self.no_save_g =  False\n",
    "        self.no_save_d =  False\n",
    "\n",
    "        self.num_epochs =  500\n",
    "        self.batch_size =  8\n",
    "        self.g_lrn_rate =  0.0000001\n",
    "        self.d_lrn_rate =  0.0000001\n",
    "\n",
    "        self.no_pretraining =  False\n",
    "        self.g_pretraining_epochs =  5\n",
    "        self.d_pretraining_epochs =  5\n",
    "        self.use_sgd =  False\n",
    "        self.conditional_freezing =  False\n",
    "        self.label_smoothing =  False\n",
    "        self.feature_matching =  False\n",
    "ARGS = ARGS()\n",
    "# ARGS = ARG_PARSER.parse_args()\n",
    "MAX_SEQ_LEN = TOTAL_FRAMES # todo\n",
    "BATCH_SIZE = ARGS.batch_size\n",
    "\n",
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
