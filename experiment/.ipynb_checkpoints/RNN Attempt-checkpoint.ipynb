{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "motion[:250]\n",
    "NUM_BODY_PARTS = 13\n",
    "TOTAL_FRAMES = 250\n",
    "# We have 250 frames. We are going to going to take the 17 body parts, \n",
    "# and turn it into 13 (remove eyes and ears). Then 13x2 (13 body parts, 2 vectors), gets shaped to\n",
    "# 26. We then take that 26, and convert it into a 250 x 26, each frame shows a body part.\n",
    "def from_motion_to_numpy_vector(motion):\n",
    "    # For now, we only take the first person. Later we can maybe try to feed in all people, or do batches of two\n",
    "    motion_vector = np.zeros((250, NUM_BODY_PARTS * 2))\n",
    "    if len(motion) != 250:\n",
    "        print(\"We need 250 frames.\")\n",
    "    for i, frame in enumerate(motion):\n",
    "        if len(frame) > 0 and i < TOTAL_FRAMES:\n",
    "            current_frame_data = frame\n",
    "            person0 = current_frame_data[0][1:]\n",
    "            current_frame_vector = np.zeros((NUM_BODY_PARTS, 2))\n",
    "            current_body_part_idx = 0\n",
    "            for body_part_data in person0:\n",
    "                body_part = body_part_data[0]\n",
    "                if body_part not in ['left_eye', 'left_ear', 'right_eye', 'right_ear']:\n",
    "                    current_frame_vector[current_body_part_idx] = body_part_data[1]\n",
    "                    current_body_part_idx = current_body_part_idx + 1\n",
    "            motion_vector[i] = current_frame_vector.reshape(NUM_BODY_PARTS * 2)\n",
    "    return motion_vector\n",
    "\n",
    "def from_numpy_vector_to_motion_coordinates(motion_vector):\n",
    "    # Reshape so each element in array is an a NUM_BODY_PARTS x 2 array that has coordinates\n",
    "    return motion_vector.reshape(TOTAL_FRAMES, NUM_BODY_PARTS, 2)\n",
    "\n",
    "class LetsDanceDataset(torch.utils.data.Dataset):\n",
    "    categories_hash = {'tango': 0, 'break': 1, 'swing': 2,'quickstep': 3,\n",
    "                  'foxtrot': 4,'pasodoble': 5,'tap': 6,'samba': 7,'flamenco': 8,\n",
    "                  'ballet': 9,'rumba': 10,'waltz': 11,'cha': 12,'latin': 13,\n",
    "                  'square': 14,'jive': 15}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # For this first test, we are just using Latin dances\n",
    "        with open('./dance-frame-list.json') as f:\n",
    "            frames_index = json.load(f)\n",
    "                    \n",
    "        latin_dances = list(filter(lambda dance: dance[0] == 'latin' and dance[2] >= TOTAL_FRAMES, frames_index))\n",
    "        \n",
    "        self.data = np.zeros((len(latin_dances), TOTAL_FRAMES, NUM_BODY_PARTS * 2))\n",
    "        self.metadata = latin_dances\n",
    "        \n",
    "        for i, dance in enumerate(latin_dances):\n",
    "            [category, dance_id, frames] = dance\n",
    "            current_frame_path = \"{}{}/{}.json\".format(self.root_dir, category, dance_id)\n",
    "            with open(current_frame_path) as f:\n",
    "                motion = json.load(f)\n",
    "            self.data[i] = from_motion_to_numpy_vector(motion)\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def getitem_metadata(self, index):\n",
    "        return self.metadata[index]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_frame_path = \"../densepose/full/latin/QmL0QYsctV0_030.json\"\n",
    "with open(current_frame_path) as f:\n",
    "    motion = json.load(f)\n",
    "        \n",
    "# for now, just return person 1 on the first frame, \n",
    "# just to see if it generates any stick figures\n",
    "# motion = np.array([pose_to_numpy(frame)[0].reshape(34) for frame in motion if len(frame) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['person0',\n",
       "  ['nose', [1051.7425537109375, 74.26245880126953]],\n",
       "  ['left_eye', [1049.74853515625, 56.2840690612793]],\n",
       "  ['right_eye', [1045.760498046875, 56.2840690612793]],\n",
       "  ['left_ear', [1007.874755859375, 54.286468505859375]],\n",
       "  ['right_ear', [1011.8627319335938, 53.28767013549805]],\n",
       "  ['left_shoulder', [1033.796630859375, 122.204833984375]],\n",
       "  ['right_shoulder', [979.9589233398438, 107.22283935546875]],\n",
       "  ['left_elbow', [1061.7125244140625, 203.1075897216797]],\n",
       "  ['right_elbow', [1025.8206787109375, 195.1171875]],\n",
       "  ['left_wrist', [1098.601318359375, 213.0955810546875]],\n",
       "  ['right_wrist', [1099.5982666015625, 209.1003875732422]],\n",
       "  ['left_hip', [1032.7996826171875, 318.96832275390625]],\n",
       "  ['right_hip', [992.9197998046875, 317.9695129394531]],\n",
       "  ['left_knee', [1087.6343994140625, 465.7918395996094]],\n",
       "  ['right_knee', [1057.7244873046875, 466.7906494140625]],\n",
       "  ['left_ankle', [1047.7545166015625, 649.5709228515625]],\n",
       "  ['right_ankle', [1022.8296508789062, 663.5541381835938]]],\n",
       " ['person1',\n",
       "  ['nose', [669.4735717773438, 185.07545471191406]],\n",
       "  ['left_eye', [678.4266357421875, 177.0872039794922]],\n",
       "  ['right_eye', [660.5205078125, 177.0872039794922]],\n",
       "  ['left_ear', [691.35888671875, 181.08132934570312]],\n",
       "  ['right_ear', [646.593505859375, 185.07545471191406]],\n",
       "  ['left_shoulder', [716.2285766601562, 226.01524353027344]],\n",
       "  ['right_shoulder', [638.6351928710938, 253.97412109375]],\n",
       "  ['left_elbow', [740.1034545898438, 290.9197692871094]],\n",
       "  ['right_elbow', [628.6873168945312, 326.8669128417969]],\n",
       "  ['left_wrist', [753.03564453125, 351.8302001953125]],\n",
       "  ['right_wrist', [612.770751953125, 400.75823974609375]],\n",
       "  ['left_hip', [719.212890625, 359.8184509277344]],\n",
       "  ['right_hip', [669.4735717773438, 362.81402587890625]],\n",
       "  ['left_knee', [737.1190795898438, 470.6554260253906]],\n",
       "  ['right_knee', [681.4110107421875, 470.6554260253906]],\n",
       "  ['left_ankle', [735.1295166015625, 644.39990234375]],\n",
       "  ['right_ankle', [716.2285766601562, 582.4909057617188]]]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person0 = motion[0]\n",
    "person0\n",
    "# Bounding box idea\n",
    "# try to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((12, 2))\n",
    "a[0] = [1,2]\n",
    "a.reshape(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(points):\n",
    "    plt.xlim(0, 1980)\n",
    "    plt.ylim(-1000, 0)\n",
    "    reshaped = points.reshape(NUM_BODY_PARTS, 2)\n",
    "    x = reshaped[:,0]\n",
    "    y = reshaped[:,1]\n",
    "    plt.scatter(x, -y, s=10, marker='.',)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-f2f6ce1324d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLetsDanceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-98f16134a772>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mlatin_dances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'latin'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mTOTAL_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatin_dances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTOTAL_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_BODY_PARTS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatin_dances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "dataloader = LetsDanceDataset()\n",
    "dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
