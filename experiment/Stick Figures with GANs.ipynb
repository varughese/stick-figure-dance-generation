{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "NUM_BODY_PARTS = 13\n",
    "TOTAL_FRAMES = 250\n",
    "BATCH_SIZE = 1\n",
    "# We have 250 frames. We are going to going to take the 17 body parts, \n",
    "# and turn it into 13 (remove eyes and ears). Then 13x2 (13 body parts, 2 vectors), gets shaped to\n",
    "# 26. We then take that 26, and convert it into a 250 x 26, each frame shows a body part.\n",
    "def from_motion_to_numpy_vector(motion):\n",
    "    # For now, we only take the first person. Later we can maybe try to feed in all people, or do batches of two\n",
    "    motion_vector = np.zeros((250, NUM_BODY_PARTS * 2))\n",
    "    if len(motion) < 250:\n",
    "        print(\"We need 250 frames.\")\n",
    "    for i, frame in enumerate(motion):\n",
    "        if len(frame) > 0 and i < TOTAL_FRAMES:\n",
    "            current_frame_data = frame\n",
    "            person0 = current_frame_data[0][1:]\n",
    "            current_frame_vector = np.zeros((NUM_BODY_PARTS, 2))\n",
    "            current_body_part_idx = 0\n",
    "            for body_part_data in person0:\n",
    "                body_part = body_part_data[0]\n",
    "                if body_part not in ['left_eye', 'left_ear', 'right_eye', 'right_ear']:\n",
    "                    current_frame_vector[current_body_part_idx] = body_part_data[1]\n",
    "                    current_body_part_idx = current_body_part_idx + 1\n",
    "            motion_vector[i] = current_frame_vector.reshape(NUM_BODY_PARTS * 2)\n",
    "    return motion_vector\n",
    "\n",
    "def from_numpy_vector_to_motion_coordinates(motion_vector):\n",
    "    # Reshape so each element in array is an a NUM_BODY_PARTS x 2 array that has coordinates\n",
    "    return motion_vector.reshape(TOTAL_FRAMES, NUM_BODY_PARTS, 2)\n",
    "\n",
    "class LetsDanceDataset(torch.utils.data.Dataset):\n",
    "    categories_hash = {'tango': 0, 'break': 1, 'swing': 2,'quickstep': 3,\n",
    "                  'foxtrot': 4,'pasodoble': 5,'tap': 6,'samba': 7,'flamenco': 8,\n",
    "                  'ballet': 9,'rumba': 10,'waltz': 11,'cha': 12,'latin': 13,\n",
    "                  'square': 14,'jive': 15}\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        category = 'latin'\n",
    "\n",
    "        # For this first test, we are just using Latin dances\n",
    "        with open('./dance-frame-list.json') as f:\n",
    "            frames_index = json.load(f)\n",
    "                    \n",
    "        latin_dances = list(filter(lambda dance: dance[0] == 'latin' and dance[2] >= TOTAL_FRAMES, frames_index))\n",
    "        \n",
    "        self.data = np.zeros((len(latin_dances), TOTAL_FRAMES, NUM_BODY_PARTS * 2))\n",
    "        self.metadata = latin_dances\n",
    "        \n",
    "        for i, dance in enumerate(latin_dances):\n",
    "            [category, dance_id, frames] = dance\n",
    "            current_frame_path = \"{}{}/{}.json\".format(root_dir, category, dance_id)\n",
    "            with open(current_frame_path) as f:\n",
    "                motion = json.load(f)\n",
    "            self.data[i] = from_motion_to_numpy_vector(motion)\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def getitem_metadata(self, index):\n",
    "        return self.metadata[index]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        with_batch_size = np.zeros((1, data.shape[0], data.shape[1]))\n",
    "        with_batch_size[0] = data\n",
    "        in_frames = with_batch_size\n",
    "        out_frames = np.zeros_like(in_frames)\n",
    "        out_frames[:-1] = in_frames[1:]\n",
    "        out_frames[-1] = in_frames[0]\n",
    "        return torch.from_numpy(in_frames), torch.from_numpy(out_frames)\n",
    "    \n",
    "dataloader = LetsDanceDataset('../densepose/full/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[878.4224, 253.5436, 777.6656,  ..., 737.1992, 921.3189, 712.2170],\n",
       "          [877.6123, 253.3364, 781.7117,  ..., 739.6100, 919.5688, 711.6518],\n",
       "          [901.2580, 264.8150, 791.5924,  ..., 737.7031, 931.1667, 724.7061],\n",
       "          ...,\n",
       "          [375.4109, 242.3493, 421.2815,  ..., 739.9741, 481.1126, 739.9741],\n",
       "          [391.2712, 240.5370, 433.2376,  ..., 741.4198, 483.1975, 734.4214],\n",
       "          [408.1478, 245.0546, 458.0829,  ..., 735.4467, 449.0946, 674.5222]]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[[878.4224, 253.5436, 777.6656,  ..., 737.1992, 921.3189, 712.2170],\n",
       "          [877.6123, 253.3364, 781.7117,  ..., 739.6100, 919.5688, 711.6518],\n",
       "          [901.2580, 264.8150, 791.5924,  ..., 737.7031, 931.1667, 724.7061],\n",
       "          ...,\n",
       "          [375.4109, 242.3493, 421.2815,  ..., 739.9741, 481.1126, 739.9741],\n",
       "          [391.2712, 240.5370, 433.2376,  ..., 741.4198, 483.1975, 734.4214],\n",
       "          [408.1478, 245.0546, 458.0829,  ..., 735.4467, 449.0946, 674.5222]]],\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
