{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-RNN-GAN\n",
    "http://mogren.one/publications/2016/c-rnn-gan/mogren2016crnngan.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# https://github.com/cjbayron/c-rnn-gan.pytorch/blob/master/train_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, features, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.features = features\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=(features*2), out_features=hidden_size)\n",
    "        self.lstm1 = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_size, out_features=features)\n",
    "        \n",
    "    def forward(self, z, states):\n",
    "        z = z.to(device)\n",
    "        batch_size, seq_len, num_feats = z.shape\n",
    "        z = torch.split(z, 1, dim=1)\n",
    "        z = [z_step.squeeze(dim=1) for z_step in z]\n",
    "        \n",
    "        prev_gen = torch.empty([batch_size, num_feats]).uniform_()\n",
    "        prev_gen = prev_gen.to(device)\n",
    "        \n",
    "        state1, state2 = states\n",
    "        gen_feats = []\n",
    "        for z_step in z:\n",
    "            concat_in = torch.cat((z_step, prev_gen), dim=-1)\n",
    "            out = F.relu(self.fc1(concat_in))\n",
    "            h1, c1 = self.lstm1(out, state1)\n",
    "            h1 = self.dropout(h1)\n",
    "            h2, c2 = self.lstm2(h1, state2)\n",
    "            prev_gen = self.fc2(h2)\n",
    "            gen_feats.append(prev_gen)\n",
    "            state1 = (h1, c1)\n",
    "            state2 = (h2, c2)\n",
    "        \n",
    "        # seq_len * (batch_size * num_feats) -> (batch_size * seq_len * num_feats)\n",
    "        gen_feats = torch.stack(gen_feats, dim=1)\n",
    "        \n",
    "        states = (state1, state2)\n",
    "        return gen_feats, states\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = ( (weight.new(batch_size, self.hidden_size).zero_().to(device),\n",
    "                   weight.new(batch_size, self.hidden_size).zero_().to(device)),\n",
    "                   (weight.new(batch_size, self.hidden_size).zero_().to(device),\n",
    "                   weight.new(batch_size, self.hidden_size).zero_().to(device)) )\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, features, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "        self.dropout = nn.Dropout(p=.5)\n",
    "        self.lstm = nn.LSTM(input_size=features, hidden_size=hidden_size,\n",
    "                           num_layers=self.num_layers, batch_first=True, dropout=0.5,\n",
    "                           bidirectional=True)\n",
    "    \n",
    "        self.fc = nn.Linear(in_features=(2*hidden_size), out_features=1)\n",
    "        \n",
    "    def forward(self, sequence, state):\n",
    "        sequence = sequence.to(device)\n",
    "        drop_in = self.dropout(sequence)\n",
    "        \n",
    "        lstm_out, state = self.lstm(drop_in, state)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        num_dims = len(out.shape)\n",
    "        reduction_dims = tuple(range(1, num_dims))\n",
    "        out = torch.mean(out, dim=reduction_dims)\n",
    "        \n",
    "        return out, lstm_out, state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        layer_mult = 2\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers * layer_mult, batch_size, self.hidden_size).zero_().to(device),\n",
    "                 weight.new(self.num_layers * layer_mult, batch_size, self.hidden_size).zero_().to(device))\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "class DLoss(nn.Module):\n",
    "    ''' C-RNN-GAN discriminator loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits_real, logits_gen):\n",
    "        ''' Discriminator loss\n",
    "        logits_real: logits from D, when input is real\n",
    "        logits_gen: logits from D, when input is from Generator\n",
    "        '''\n",
    "        logits_real = torch.clamp(logits_real, EPSILON, 1.0)\n",
    "        d_loss_real = -torch.log(logits_real)\n",
    "\n",
    "        logits_gen = torch.clamp((1 - logits_gen), EPSILON, 1.0)\n",
    "        d_loss_gen = -torch.log(logits_gen)\n",
    "\n",
    "        batch_loss = d_loss_real + d_loss_gen\n",
    "        return torch.mean(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_logits_gen, _, _ = dmodel(g_feats, d_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, trying to output numbers with the function `f(n) = 2*f(n-1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "npdata = np.stack([2 ** np.arange(10)[:, np.newaxis] * np.random.rand() for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TensorDataset(torch.from_numpy(npdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(dataloader))[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of data loader is a sequence. Thats it We just put this into a function to make a train set and validation set easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_dataloader(seq_len, batch_size, num_sample):\n",
    "    ''' Dummy data generator (for debugging purposes)\n",
    "    '''\n",
    "    # the following code generates random data of numbers\n",
    "    # where each number is twice the prev number\n",
    "    np_data = np.stack([(2 ** np.arange(seq_len))[:, np.newaxis] \\\n",
    "                        * np.random.rand() for i in range(num_sample)])\n",
    "\n",
    "    data = TensorDataset(torch.from_numpy(np_data))\n",
    "    return DataLoader(data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader = dummy_dataloader(20, 13, 7)\n",
    "val_dataloader = dummy_dataloader(20, 13, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'g': Generator(features=1, hidden_size=100),\n",
    "    'd': Discriminator(features=1, hidden_size=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_LRN_RATE = 0.001\n",
    "D_LRN_RATE = 0.001\n",
    "optimizer = {\n",
    "    'g': optim.Adam(model['g'].parameters(), G_LRN_RATE),\n",
    "    'd': optim.Adam(model['d'].parameters(), D_LRN_RATE)\n",
    "}\n",
    "criterion = {\n",
    "    'g': nn.MSELoss(reduction='sum'),\n",
    "    'd': DLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
